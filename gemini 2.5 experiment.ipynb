{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenliu-DH/Gemini-2.5-flash_-SFT-DPO_experiment/blob/main/gemini%202.5%20experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4-kxwz23nzr"
      },
      "source": [
        "# Supervised Fine Tuning with Gemini 2.5 Flash for Article Summarization\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/sft_gemini_summarization.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fsft_gemini_summarization.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/sft_gemini_summarization.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/sft_gemini_summarization.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/sft_gemini_summarization.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/sft_gemini_summarization.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/sft_gemini_summarization.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/sft_gemini_summarization.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/sft_gemini_summarization.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veTVRUGFoggy"
      },
      "source": [
        "# **Supervised Fine Tuning & DPO with Gemini 2.5 Flash**\n",
        "  reference:https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning\n",
        "\n",
        "  data:https://storage.googleapis.com/dh-search-science-shared/roman/qc-train-data/Thomas/export_145692_project-145692-at-2025-06-03-09-50-c307134e.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vpa8F0yrN-l"
      },
      "source": [
        "<h1 style=\"font-size: 40px; font-weight: bold; color: #1a73e8;\">\n",
        "  Project Overview: E-commerce Search Relevance Fine-Tuning\n",
        "</h1>\n",
        "\n",
        "This project demonstrates an advanced workflow for optimizing **Gemini 2.5 Flash** to perform high-accuracy product relevance labeling. By leveraging the **Amazon ESCI (Exact, Substitute, Complement, Irrelevant)** scale, we transform raw search query data into a structured classification system for e-commerce search engines.\n",
        "\n",
        "### **Key Components**\n",
        "* **Model Architecture**: Utilizes **Gemini 2.5 Flash** for efficient, high-speed inference and labeling.\n",
        "* **Dataset Integration**: Processes large-scale JSON data hosted on **Google Cloud Storage (GCS)**, featuring localized product names and multi-language attributes.\n",
        "* **Two-Stage Optimization**:\n",
        "    * **Stage 1: Supervised Fine-Tuning (SFT)** to teach the model the base ESCI definitions and formatting requirements.\n",
        "    * **Stage 2: Direct Preference Optimization (DPO)** to refine the model's decision-making on \"hard negatives\" and cross-category errors.\n",
        "* **Automated Evaluation**: Includes a comprehensive error analysis pipeline that calculates **Score Differences (Distance)** to quantify the severity of misclassifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3N6hKLfrBfc"
      },
      "source": [
        "<h1 style=\"font-size: 36px; font-weight: bold; color: #34a853;\">\n",
        "  Technical Objectives\n",
        "</h1>\n",
        "\n",
        "The primary goal of this implementation is to surpass baseline model performance in complex search scenarios by aligning AI outputs with expert human annotations through a rigorous feedback loop.\n",
        "\n",
        "### **Strategic Goals**\n",
        "1.  **High-Precision Labeling**: Achieve over 90% accuracy across the four ESCI grades, specifically reducing \"fatal\" errors such as labeling an **Exact** match as **Irrelevant**.\n",
        "2.  **Cross-Lingual Consistency**: Ensure the model maintains high labeling quality across different languages (e.g., Arabic, English) by utilizing the `language` attribute during training and evaluation.\n",
        "3.  **Instruction Adherence**: Enforce strict output constraints (Single Word Labels) without the need for additional post-processing or regex cleaning.\n",
        "4.  **DPO Preference Alignment**:\n",
        "    * Identify \"Hard Negatives\" from the SFT training set where the model's confidence is misplaced.\n",
        "    * Construct a **Chosen/Rejected** dataset based on the **Score Difference (score_diff)** metric to penalize high-distance errors more heavily.\n",
        "5.  **Scalable Inference**: Implement **Vertex AI Batch Prediction** to handle thousands of queries asynchronously, optimizing both cost and processing time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llEFILYz2aye"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo2rh4cC2e1r"
      },
      "source": [
        "### Install Gen AI SDK and other required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d8bad6906dc"
      },
      "source": [
        "The new Google Gen AI SDK provides a unified interface to Gemini through both the Gemini Developer API and the Gemini API on Vertex AI. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l_ok3vdw2cyf"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet google-genai google-cloud-aiplatform rouge_score plotly jsonlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0HMlz-MD9Yt"
      },
      "source": [
        "## Step0: Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "86VNaqlgD9rK"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKRPFNzWJLVY"
      },
      "source": [
        "- If you are running this notebook in a local development environment:\n",
        "  - Install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
        "  - Obtain authentication credentials. Create local credentials by running the following command and following the oauth2 flow (read more about the command [here](https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login)):\n",
        "\n",
        "    ```bash\n",
        "    gcloud auth application-default login\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8CI-TcqD06L"
      },
      "source": [
        "## Step1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rerpHL_eEG8D"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# For data handling.\n",
        "import jsonlines\n",
        "import pandas as pd\n",
        "\n",
        "# For visualization.\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# For fine tuning Gemini model.\n",
        "import vertexai\n",
        "from google import genai\n",
        "\n",
        "# For extracting vertex experiment details.\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform.metadata import context\n",
        "from google.cloud.aiplatform.metadata import utils as metadata_utils\n",
        "from google.genai import types\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# For evaluation metric computation.\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DB5XtNNBi1Lq"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBY9nK3qEJLk"
      },
      "source": [
        "## Step2: Set Google Cloud project information and initialize Vertex AI and Gen AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VpzmI1K61Tn2"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"search-science-shared-7335\"\n",
        "REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7MZmIZvlQUhy"
      },
      "outputs": [],
      "source": [
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUEloBlsCPFr"
      },
      "source": [
        "## Step3: Create Dataset in correct format\n",
        "\n",
        "The dataset used to tune a foundation model needs to include examples that align with the task that you want the model to perform. Structure your training dataset in a text-to-text format. Each record, or row, in the dataset contains the input text (also referred to as the prompt) which is paired with its expected output from the model. Supervised tuning uses the dataset to teach the model to mimic a behavior, or task, you need by giving it hundreds of examples that illustrate that behavior.\n",
        "\n",
        "Your dataset size depends on the task, and follows the recommendation mentioned in the `Overview` section. The more examples you provide in your dataset, the better the results.\n",
        "\n",
        "### Dataset format\n",
        "\n",
        "Training data should be structured within a JSONL file located at a Google Cloud Storage (GCS) URI. Each line (or row) of the JSONL file must adhere to a specific schema: It should contain a `contents` array, with objects inside defining a `role` (either \"user\" for user input or \"model\" for model output) and `parts`, containing the input data. For example, a valid data row would look like this:\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"contents\": [\n",
        "    {\n",
        "      \"role\": \"user\", # This indicates input content\n",
        "      \"parts\": [\n",
        "        {\n",
        "          \"text\": \"How are you?\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\", # This indicates target content\n",
        "      \"parts\": [ # text only\n",
        "        {\n",
        "          \"text\": \"I am good, thank you!\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ] #  ... repeat \"user\", \"model\" for multi turns.\n",
        "}\n",
        "```\n",
        "\n",
        "Refer to the public [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-prepare#about-datasets) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TglfI3tQr2oZ"
      },
      "source": [
        "To run a tuning job, you need to upload one or more datasets to a Cloud Storage bucket. You can either create a new Cloud Storage bucket or use an existing one to store dataset files. The region of the bucket doesn't matter, but we recommend that you use a bucket that's in the same Google Cloud project where you plan to tune your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzWc0czenfh2"
      },
      "source": [
        "Step3: transform the format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jeeDoRVBlrVe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def process_and_export_relevance_data(source_path, agreement_threshold, file_prefix, user_content_template, split_strategy=1):\n",
        "    \"\"\"\n",
        "    Reads, cleans, filters, and exports search relevance data.\n",
        "\n",
        "    Args:\n",
        "        source_path (str): The file path of the source JSON.\n",
        "        agreement_threshold (int): The threshold for agreement filtering (e.g., 100).\n",
        "        file_prefix (str): The prefix for output files (e.g., \"sft_v1\").\n",
        "        user_content_template (str): The user prompt template, must include {sq} and {pnl} placeholders.\n",
        "        split_strategy (int):\n",
        "            1 for Random row-level split (70/15/15).\n",
        "            2 for Query-level split (70/15/15 on unique queries).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_df, val_df, test_df) if successful, otherwise (None, None, None).\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Reading data from: {source_path}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. Data Loading\n",
        "    # ==========================================\n",
        "    try:\n",
        "        df_raw = pd.read_json(source_path)\n",
        "        records = df_raw.to_dict(orient='records')\n",
        "    except Exception as e:\n",
        "        print(f\"Pandas read failed, attempting manual load: {e}\")\n",
        "        records = []\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. Data Cleaning and Field Extraction\n",
        "    # ==========================================\n",
        "    extracted_rows = []\n",
        "    if records:\n",
        "        for item in records:\n",
        "            item_data = item.get('data', {})\n",
        "\n",
        "            # Extract annotation results (Result)\n",
        "            result_value = None\n",
        "            annotations = item.get('annotations', [])\n",
        "            if annotations:\n",
        "                for res in annotations[0].get('result', []):\n",
        "                    if res.get('from_name') == 'relevance':\n",
        "                        choices = res.get('value', {}).get('choices', [])\n",
        "                        if choices:\n",
        "                            result_value = choices[0]\n",
        "                        break\n",
        "\n",
        "            # Extract 'agreement' field\n",
        "            agreement_val = item.get('agreement')\n",
        "\n",
        "            extracted_row = {\n",
        "                'search_query': item_data.get('search_query'),\n",
        "                'query_segment': item_data.get('traffic_segment'),\n",
        "                'product_name_local': item_data.get('name'),\n",
        "                'language': item_data.get('query_language'),\n",
        "                'description': item_data.get('description'),\n",
        "                'result': result_value,\n",
        "                'agreement': agreement_val\n",
        "            }\n",
        "            extracted_rows.append(extracted_row)\n",
        "\n",
        "    df = pd.DataFrame(extracted_rows)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No data extracted.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- Pre-processing Filters ---\n",
        "    df['agreement'] = pd.to_numeric(df['agreement'], errors='coerce').fillna(0)\n",
        "\n",
        "    # Filter by Agreement Threshold\n",
        "    original_count = len(df)\n",
        "    df = df[df['agreement'] >= agreement_threshold]\n",
        "    print(f\"Filtered by agreement >= {agreement_threshold}: {original_count} -> {len(df)} rows\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['search_query', 'product_name_local'], keep='first')\n",
        "    print(f\"Deduplicated (Query + Product Name): -> {len(df)} rows\")\n",
        "    print(f\"Final total rows for training splitting: {len(df)}\")\n",
        "\n",
        "    # --- Column Selection ---\n",
        "    required_columns = [\n",
        "        'search_query', 'query_segment', 'product_name_local', 'language',\n",
        "        'product_name_english', 'category_parent_english',\n",
        "        'master_category_english', 'result'\n",
        "    ]\n",
        "    df = df[required_columns]\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. Data Splitting (70% Train, 15% Val, 15% Test)\n",
        "    # ==========================================\n",
        "    if len(df) > 0:\n",
        "        RANDOM_STATE = 42\n",
        "\n",
        "        if split_strategy == 1:\n",
        "            print(\"Using Split Strategy 1: Random Row-level Split...\")\n",
        "            train_df = df.sample(frac=0.7, random_state=RANDOM_STATE)\n",
        "            remaining_df = df.drop(train_df.index)\n",
        "            val_df = remaining_df.sample(frac=0.5, random_state=RANDOM_STATE)\n",
        "            test_df = remaining_df.drop(val_df.index)\n",
        "\n",
        "        elif split_strategy == 2:\n",
        "            print(\"Using Split Strategy 2: Query-level Split (Zero-shot Queries)...\")\n",
        "            # Get all unique search_queries\n",
        "            unique_queries = df['search_query'].dropna().unique()\n",
        "\n",
        "            # Shuffle the order of queries\n",
        "            np.random.seed(RANDOM_STATE)\n",
        "            np.random.shuffle(unique_queries)\n",
        "\n",
        "            # Calculate the split indices for 70/15/15\n",
        "            n_queries = len(unique_queries)\n",
        "            train_idx = int(n_queries * 0.7)\n",
        "            val_idx = int(n_queries * 0.85)\n",
        "\n",
        "            # Distribute queries into different sets\n",
        "            train_queries = unique_queries[:train_idx]\n",
        "            val_queries = unique_queries[train_idx:val_idx]\n",
        "            test_queries = unique_queries[val_idx:]\n",
        "\n",
        "            # Allocate original data to train/val/test based on the queries\n",
        "            train_df = df[df['search_query'].isin(train_queries)]\n",
        "            val_df = df[df['search_query'].isin(val_queries)]\n",
        "            test_df = df[df['search_query'].isin(test_queries)]\n",
        "\n",
        "            print(f\"Unique Queries split - Train: {len(train_queries)}, Val: {len(val_queries)}, Test: {len(test_queries)}\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"split_strategy must be 1 or 2\")\n",
        "\n",
        "        print(f\"Final Row Split sizes - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # 4. Export to CSV and JSONL\n",
        "        # ==========================================\n",
        "        # Name files using the file_prefix parameter\n",
        "        train_df.to_csv(f\"{file_prefix}_train_samples.csv\", index=False)\n",
        "        val_df.to_csv(f\"{file_prefix}_val_samples.csv\", index=False)\n",
        "        test_df.to_csv(f\"{file_prefix}_test_samples.csv\", index=False)\n",
        "\n",
        "        def create_sft_format(dataframe):\n",
        "            instances = []\n",
        "            for _, row in dataframe.iterrows():\n",
        "                sq = str(row['search_query']) if row['search_query'] is not None else \"\"\n",
        "                pnl = str(row['product_name_local']) if row['product_name_local'] is not None else \"\"\n",
        "                res = str(row['result']) if row['result'] is not None else \"\"\n",
        "\n",
        "                try:\n",
        "                    user_content = user_content_template.format(sq=sq, pnl=pnl)\n",
        "                except KeyError as e:\n",
        "                    print(f\"Warning: Template key error {e}. Using fallback format.\")\n",
        "                    user_content = f\"Query: {sq} Product: {pnl}\"\n",
        "\n",
        "                model_content = f\"{res}\"\n",
        "\n",
        "                instance = {\n",
        "                    \"contents\": [\n",
        "                        {\"role\": \"user\", \"parts\": [{\"text\": user_content}]},\n",
        "                        {\"role\": \"model\", \"parts\": [{\"text\": model_content}]}\n",
        "                    ]\n",
        "                }\n",
        "                instances.append(instance)\n",
        "            return instances\n",
        "\n",
        "        # Convert and save as JSONL\n",
        "        save_jsonlines = lambda filename, data: [open(filename, 'w').write('\\n'.join([json.dumps(i, ensure_ascii=False) for i in data]))]\n",
        "\n",
        "        save_jsonlines(f\"{file_prefix}_train_samples.jsonl\", create_sft_format(train_df))\n",
        "        save_jsonlines(f\"{file_prefix}_val_samples.jsonl\", create_sft_format(val_df))\n",
        "        save_jsonlines(f\"{file_prefix}_test_samples.jsonl\", create_sft_format(test_df))\n",
        "\n",
        "        print(f\"All tasks completed. Files generated with prefix '{file_prefix}'.\")\n",
        "\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "    else:\n",
        "        print(f\"No data remaining after filters (Agreement >= {agreement_threshold}).\")\n",
        "        return None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFjyCljcbGyy",
        "outputId": "5dcea485-b8a4-491c-f530-b1611a0602bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data from: gs://dh-search-science-shared/roman/qc-train-data/Thomas/Talabat-new-whole dataset.json\n",
            "Filtered by agreement >= 100: 10551 -> 8625 rows\n",
            "Deduplicated (Query + Product Name): -> 571 rows\n",
            "Final total rows for training splitting: 571\n",
            "Using Split Strategy 2: Query-level Split (Zero-shot Queries)...\n",
            "Unique Queries split - Train: 399, Val: 86, Test: 86\n",
            "Final Row Split sizes - Train: 399, Val: 86, Test: 86\n",
            "All tasks completed. Files generated with prefix 'experiment_old_june_data'.\n",
            "\n",
            "Function execution successful!\n",
            "Train DF shape: (399, 8)\n",
            "Val DF shape: (86, 8)\n",
            "Test DF shape: (86, 8)\n",
            "Files created: ['experiment_old_june_data_train_samples.jsonl', 'experiment_old_june_data_train_samples.csv', 'experiment_old_june_data_val_samples.csv', 'experiment_old_june_data_test_samples.csv', 'experiment_old_june_data_val_samples.jsonl', 'experiment_old_june_data_test_samples.jsonl']\n"
          ]
        }
      ],
      "source": [
        "bucket_path = \"gs://dh-search-science-shared/roman/qc-train-data/Thomas/\"\n",
        "source_file = f\"{bucket_path}Talabat-new-whole dataset.json\"\n",
        "my_user_prompt = \"You are a search relevance expert. Your task is to label ecommerce products relevance for a given query. Output ONLY the single word label. Query: {sq} Product: {pnl} Result:\"\n",
        "my_file_prefix=\"experiment_old_june_data\"\n",
        "# 3. Call the function and receive the return values\n",
        "# Set agreement_threshold to 100, file_prefix to \"experiment_en\"\n",
        "train_df, val_df, test_df = process_and_export_relevance_data(\n",
        "    source_path=source_file,\n",
        "    agreement_threshold=100,\n",
        "    file_prefix=my_file_prefix,\n",
        "    user_content_template=my_user_prompt,\n",
        "    split_strategy=2\n",
        ")\n",
        "\n",
        "# 4. Verify the results\n",
        "if train_df is not None:\n",
        "    print(\"\\nFunction execution successful!\")\n",
        "    print(f\"Train DF shape: {train_df.shape}\")\n",
        "    print(f\"Val DF shape: {val_df.shape}\")\n",
        "    print(f\"Test DF shape: {test_df.shape}\")\n",
        "\n",
        "    # Check the generated files\n",
        "    import os\n",
        "    print(\"Files created:\", [f for f in os.listdir('.') if f.startswith(my_file_prefix)])\n",
        "else:\n",
        "    print(\"Function execution failed or resulted in empty data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HLBI2ez88wA"
      },
      "outputs": [],
      "source": [
        "# Upload training data\n",
        "!gsutil cp {my_file_prefix}_train_samples.jsonl {bucket_path}\n",
        "\n",
        "# Upload validation data\n",
        "!gsutil cp {my_file_prefix}_val_samples.jsonl {bucket_path}\n",
        "\n",
        "# Check if the files actually exist\n",
        "!gsutil ls {bucket_path}{my_file_prefix}_*.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhjmRffOOPAS"
      },
      "source": [
        "## Step4: Initailize model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhhD1VWDsLat"
      },
      "source": [
        "The following Gemini text model support supervised tuning:\n",
        "\n",
        "* `gemini-2.5-flash`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL-zRl5_OVZW"
      },
      "outputs": [],
      "source": [
        "base_model = \"gemini-2.5-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieJe8yGlOtFD"
      },
      "source": [
        "## Step5: Defining the formule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4EUAmrCshT7"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"You are a search relevance expert. Your task is to label ecommerce products relevance for a given query.\n",
        "You ONLY have 4 possible relevance grades on the Amazon ESCI scale: Exact, Substitute, Complement, and Irrelevant.\n",
        "\n",
        "PLEASE DON'T ADD ANY EXPLANATION. Output ONLY the single word label. Do NOT use brackets.\n",
        "\n",
        "# Format\n",
        "Query: Query\n",
        "Product: Product Name\n",
        "Result: Grade\n",
        "\n",
        "# Examples (Demonstration)\n",
        "\n",
        "Query: mince beef\n",
        "Product: Beef Mince Australia, 1kg\n",
        "Result: Exact\n",
        "\n",
        "Query: mince beef\n",
        "Product: Americana Ground Mutton Mince 400 g\n",
        "Result: Substitute\n",
        "\n",
        "Query: mince beef\n",
        "Product: Burger Bun (Bread)\n",
        "Result: Complement\n",
        "\n",
        "Query: mince beef\n",
        "Product: Fancy Feast Gravy Lovers Beef Feast Cat Wet Food\n",
        "Result: Irrelevant\n",
        "\n",
        "# Your Task\n",
        "\n",
        "Query: {query}\n",
        "Product: {product}\n",
        "Result:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDTpk2hi3Akq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, accuracy_score, cohen_kappa_score\n",
        "\n",
        "def run_evaluation(test_df, model_name, prompt_template, config=None, output_csv_path=\"evaluation_results.csv\"):\n",
        "    \"\"\"\n",
        "    1. test_df (pd.DataFrame): DataFrame containing test data (must have 'search_query', 'product_name_local', 'result' columns)\n",
        "    2. model_name (str): Model name (e.g., \"gemini-1.5-flash-002\" or SFT Endpoint resource path)\n",
        "    3. prompt_template (str): String template containing {query} and {product}\n",
        "    4. config (dict): Model generation parameters (Optional)\n",
        "    5. output_csv_path (str): The file path where the resulting CSV will be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Parameter Settings\n",
        "    if config is None:\n",
        "        config = {\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_output_tokens\": 128,\n",
        "        }\n",
        "\n",
        "    # 2. Data Cleaning\n",
        "    print(f\"Original data size: {len(test_df)}\")\n",
        "    df_clean = test_df.dropna().reset_index(drop=True)\n",
        "    print(f\"Cleaned data size:  {len(df_clean)}\")\n",
        "\n",
        "    # 3. Initialize Variables\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    valid_indices = []\n",
        "    labels = ['Exact', 'Substitute', 'Complement', 'Irrelevant']\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Starting Prediction with Model: {model_name}\")\n",
        "\n",
        "    # 4. Start Timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 5. Prediction Loop\n",
        "    for index, row in tqdm(df_clean.iterrows(), total=len(df_clean), desc=\"Predicting\"):\n",
        "        try:\n",
        "            # Fill in Prompt\n",
        "            prompt_text = prompt_template.format(\n",
        "                query=row['search_query'],\n",
        "                product=row['product_name_local']\n",
        "            )\n",
        "\n",
        "            # Call Model (Assuming 'client' is defined globally or passed somehow, keeping your original logic)\n",
        "            response = client.models.generate_content(\n",
        "                model=model_name,\n",
        "                contents=prompt_text,\n",
        "                config=config,\n",
        "            )\n",
        "\n",
        "            # Post-process Results\n",
        "            raw_output = response.text.strip()\n",
        "            clean_output = raw_output.replace(\"[result] is\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").strip()\n",
        "\n",
        "            # Normalize\n",
        "            pred_norm = clean_output.capitalize()\n",
        "            gt_norm = str(row['result']).strip().capitalize()\n",
        "\n",
        "            y_pred.append(pred_norm)\n",
        "            y_true.append(gt_norm)\n",
        "            valid_indices.append(index)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError at index {index}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # 6. End Timing\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # 7. Calculate Metrics\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # ==========================================\n",
        "    # ▼ Export to CSV ▼\n",
        "    # ==========================================\n",
        "    # Filter the original dataframe to only include rows that were successfully processed\n",
        "    df_results = df_clean.iloc[valid_indices].copy()\n",
        "\n",
        "    # Add the predictions and cleaned ground truth to the dataframe\n",
        "    df_results['ground_truth'] = y_true\n",
        "    df_results['prediction'] = y_pred\n",
        "\n",
        "    # Add a column to easily see if the prediction was correct\n",
        "    df_results['is_correct'] = df_results['ground_truth'] == df_results['prediction']\n",
        "\n",
        "    # Save to CSV\n",
        "    try:\n",
        "        df_results.to_csv(output_csv_path, index=False)\n",
        "        print(f\"\\n[Success] Results saved to CSV: {output_csv_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[Error] Failed to save CSV: {e}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # ▼ Classification Report ▼\n",
        "    # ==========================================\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Time: {elapsed_time:.2f}s ({elapsed_time / len(y_pred):.4f}s/sample)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.2%}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, labels=labels, zero_division=0))\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    interpretation = \"Fair\"\n",
        "    if kappa > 0.8: interpretation = \"Almost Perfect\"\n",
        "    elif kappa > 0.6: interpretation = \"Substantial\"\n",
        "    elif kappa > 0.4: interpretation = \"Moderate\"\n",
        "\n",
        "    print(f\"Cohen's Kappa Score: {kappa:.4f} ({interpretation})\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # ==========================================\n",
        "    # ▼ Return Value: Includes original data (data) ▼\n",
        "    # ==========================================\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"accuracy\": acc,\n",
        "        \"kappa\": kappa,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"data\": df_clean,\n",
        "        \"valid_indices\": valid_indices,\n",
        "        \"results_df\": df_results # Optionally return the results dataframe as well\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_by_query_segment(results_dict):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance grouped by 'query_segment'.\n",
        "\n",
        "    Parameters:\n",
        "    results_dict (dict): The output dictionary from the run_evaluation function.\n",
        "                         It must contain the 'results_df' key with 'query_segment',\n",
        "                         'ground_truth', and 'prediction' columns.\n",
        "    \"\"\"\n",
        "    # 1. Extract the results dataframe\n",
        "    if 'results_df' in results_dict:\n",
        "        df = results_dict['results_df']\n",
        "    else:\n",
        "        raise KeyError(\"The 'results_df' key is missing in results_dict.\")\n",
        "\n",
        "    # 2. Check if 'query_segment' exists in the dataframe\n",
        "    if 'query_segment' not in df.columns:\n",
        "        raise ValueError(\"The column 'query_segment' is not present in the data.\")\n",
        "\n",
        "    model_name = results_dict.get('model', 'Unknown Model')\n",
        "    labels = ['Exact', 'Substitute', 'Complement', 'Irrelevant']\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Evaluation by Query Segment for Model: {model_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 3. Get unique query segments (e.g., HEAD, TORSO, TAIL)\n",
        "    segments = df['query_segment'].dropna().unique()\n",
        "\n",
        "    # 4. Iterate over each segment and calculate metrics\n",
        "    for segment in segments:\n",
        "        print(f\"\\n>>> Query Segment: [ {segment} ]\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Filter data for the current segment\n",
        "        df_seg = df[df['query_segment'] == segment]\n",
        "\n",
        "        if len(df_seg) == 0:\n",
        "            continue\n",
        "\n",
        "        y_true = df_seg['ground_truth']\n",
        "        y_pred = df_seg['prediction']\n",
        "\n",
        "        # Calculate Accuracy and Cohen's Kappa Score\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        kappa = cohen_kappa_score(y_true, y_pred, labels=labels)\n",
        "\n",
        "        # Determine Kappa interpretation\n",
        "        interpretation = \"Poor\"\n",
        "        if kappa > 0.8:\n",
        "            interpretation = \"Almost Perfect\"\n",
        "        elif kappa > 0.6:\n",
        "            interpretation = \"Substantial\"\n",
        "        elif kappa > 0.4:\n",
        "            interpretation = \"Moderate\"\n",
        "        elif kappa > 0.2:\n",
        "            interpretation = \"Fair\"\n",
        "\n",
        "        # Print the metrics\n",
        "        print(f\"Number of samples : {len(df_seg)}\")\n",
        "        print(f\"Accuracy          : {acc:.2%}\")\n",
        "        print(f\"Cohen's Kappa     : {kappa:.4f} ({interpretation})\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred, labels=labels, zero_division=0))\n",
        "        print(\"*\" * 50)"
      ],
      "metadata": {
        "id": "-DgTqCD7kNe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zEyLDnbyCUu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(results_dict):\n",
        "    \"\"\"\n",
        "    Generates and plots a confusion matrix from the evaluation results.\n",
        "\n",
        "    Parameters:\n",
        "    results_dict (dict): A dictionary containing 'y_true', 'y_pred', and 'model' name.\n",
        "                         (Output from the run_evaluation function)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Extract data from the results dictionary\n",
        "    y_true = results_dict['y_true']\n",
        "    y_pred = results_dict['y_pred']\n",
        "    model_name = results_dict.get('model', 'Unknown Model')\n",
        "\n",
        "    # Define standard label order\n",
        "    labels = ['Exact', 'Substitute', 'Complement', 'Irrelevant']\n",
        "\n",
        "    # 2. Calculate Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # 3. Plotting\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Use Seaborn to plot heatmap\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',            # Integer formatting\n",
        "        cmap='Blues',       # Color map\n",
        "        xticklabels=labels,\n",
        "        yticklabels=labels\n",
        "    )\n",
        "\n",
        "    # Set titles and labels\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.ylabel('Actual Label (Ground Truth)')\n",
        "    plt.xlabel('Predicted Label (Model Output)')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1GdulT6zQRT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display # Import display specifically for Jupyter environment\n",
        "\n",
        "def analyze_model_errors(results_dict, output_csv_path=\"model_errors.csv\", top_n=10):\n",
        "    \"\"\"\n",
        "    Analyzes model errors, displays the most severe cases, and saves the full report to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    results_dict (dict): Output from run_evaluation.\n",
        "                         Must contain 'y_true', 'y_pred', 'data', and 'valid_indices'.\n",
        "    output_csv_path (str): The filename/path to save the error report CSV.\n",
        "    top_n (int): Number of top severe errors to display in the notebook.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The DataFrame containing all error rows.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Define ESCI score mapping (higher weights represent higher relevance)\n",
        "    score_map = {\n",
        "        'Exact': 3,\n",
        "        'Substitute': 2,\n",
        "        'Complement': 1,\n",
        "        'Irrelevant': 0\n",
        "    }\n",
        "\n",
        "    # 2. Extract Data\n",
        "    y_true = results_dict['y_true']\n",
        "    y_pred = results_dict['y_pred']\n",
        "    model_name = results_dict.get('model', 'Unknown Model')\n",
        "\n",
        "    # Ensure we can retrieve the original Query and Product info\n",
        "    if 'data' in results_dict:\n",
        "        # If valid_indices exists, use it to align rows (in case some rows were skipped due to errors)\n",
        "        if 'valid_indices' in results_dict:\n",
        "            results_df = results_dict['data'].iloc[results_dict['valid_indices']].copy()\n",
        "        else:\n",
        "            # Assume data aligns perfectly if no indices provided\n",
        "            results_df = results_dict['data'].copy()\n",
        "    else:\n",
        "        raise KeyError(\"The 'data' key is missing in results_dict. Please update run_evaluation to return 'data': df_clean\")\n",
        "\n",
        "    # Add predictions to the dataframe\n",
        "    results_df['model_prediction'] = y_pred\n",
        "    results_df['ground_truth'] = y_true\n",
        "\n",
        "    # 3. Filter for records where predictions do not match the ground truth\n",
        "    error_df = results_df[results_df['model_prediction'] != results_df['ground_truth']].copy()\n",
        "\n",
        "    # 4. Calculate scores and differences for severity analysis\n",
        "    # Use .get() to handle unknown labels (assign 0 if invalid)\n",
        "    error_df['gt_score'] = error_df['ground_truth'].map(lambda x: score_map.get(x, 0))\n",
        "    error_df['pred_score'] = error_df['model_prediction'].map(lambda x: score_map.get(x, 0))\n",
        "\n",
        "    # Calculate absolute difference (Score Distance)\n",
        "    # Example: Exact(3) vs Irrelevant(0) -> Difference of 3 (Most Severe)\n",
        "    error_df['score_diff'] = (error_df['gt_score'] - error_df['pred_score']).abs()\n",
        "\n",
        "    # 5. Sort by score difference in descending order (most severe errors at the top)\n",
        "    error_df = error_df.sort_values(by='score_diff', ascending=False)\n",
        "\n",
        "    # 6. Display Summary and Top N Errors\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(f\"Error Analysis Report: {model_name}\")\n",
        "    print(f\"Total Errors Found: {len(error_df)}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Top {top_n} Most Severe Errors:\")\n",
        "\n",
        "    # Display the top N rows nicely in Jupyter\n",
        "    display(error_df.head(top_n))\n",
        "\n",
        "    # 7. Save full report to CSV\n",
        "    if output_csv_path:\n",
        "        error_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Full error report saved to: {output_csv_path}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    return error_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGPUKZlcP69-"
      },
      "source": [
        "## Step6: Evaluation before model tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IH5iIBlcLhr"
      },
      "source": [
        "prediction without sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w94xHK5GAeeC"
      },
      "outputs": [],
      "source": [
        "base_results = run_evaluation(\n",
        "    test_df=test_df,\n",
        "    model_name=base_model,\n",
        "    prompt_template=PROMPT_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ-u8Do6A4d5"
      },
      "outputs": [],
      "source": [
        "print(\"Plotting base Model Results...\")\n",
        "plot_confusion_matrix(base_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBqQ5b0SBC7d"
      },
      "outputs": [],
      "source": [
        "base_errors = analyze_model_errors(\n",
        "    base_results,\n",
        "    output_csv_path=\"base_error_test data.csv\",\n",
        "    top_n=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMb3E0YEqL2"
      },
      "source": [
        "## Step7: Fine-tune the Model\n",
        "\n",
        " - `source_model`: Specifies the base Gemini model version you want to fine-tune.\n",
        " - `train_dataset`: Path to your training data in JSONL format.\n",
        "\n",
        "  *Optional parameters*\n",
        " - `validation_dataset`: If provided, this data is used to evaluate the model during tuning.\n",
        " - `tuned_model_display_name`: Display name for the tuned model.\n",
        " - `epochs`: The number of training epochs to run.\n",
        " - `learning_rate_multiplier`: A value to scale the learning rate during training.\n",
        " - `adapter_size` : Gemini 2.5 Flash supports Adapter length [1, 2, 4, 8], default value is 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e81137766c6"
      },
      "source": [
        "**Note: The default hyperparameter settings are optimized for optimal performance based on rigorous testing and are recommended for initial use. Users may customize these parameters to address specific performance requirements.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQM2vDBZ27b_"
      },
      "outputs": [],
      "source": [
        "# 1. Set the model display name (You can modify this freely; avoid spaces or special characters)\n",
        "tuned_model_display_name = \"gemini-2.5-flash-old data-split 2\"\n",
        "\n",
        "# 2. Set the training dataset path\n",
        "training_dataset = {\n",
        "    \"gcs_uri\": f\"{bucket_path}{my_file_prefix}_train_samples.jsonl\",\n",
        "}\n",
        "\n",
        "# 3. Set the validation dataset path\n",
        "validation_dataset = types.TuningValidationDataset(\n",
        "    gcs_uri=f\"{bucket_path}{my_file_prefix}_val_samples.jsonl\"\n",
        ")\n",
        "\n",
        "# 4. Start the fine-tuning job (This part remains unchanged; ensure base_model is defined in Step 4)\n",
        "sft_tuning_job = client.tunings.tune(\n",
        "    base_model=base_model,\n",
        "    training_dataset=training_dataset,\n",
        "    config=types.CreateTuningJobConfig(\n",
        "        tuned_model_display_name=tuned_model_display_name,\n",
        "        validation_dataset=validation_dataset,\n",
        "        # You can add parameters like epochs or learning_rate_multiplier here\n",
        "        # epochs=4,\n",
        "        # learning_rate_multiplier=1.0\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"Tuning job started: {sft_tuning_job.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEVeBCwAVCaN"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "\n",
        "def create_tuning_job(client, base_model, bucket_path, display_name, file_prefix, epochs=None, learning_rate=None):\n",
        "    \"\"\"\n",
        "    Wrapper function to launch a Gemini SFT fine-tuning job using the new SDK.\n",
        "    \"\"\"\n",
        "    train_uri = f\"{bucket_path}{file_prefix}_train_samples.jsonl\"\n",
        "    val_uri = f\"{bucket_path}{file_prefix}_val_samples.jsonl\"\n",
        "\n",
        "    print(f\"🚀 Starting tuning job for: {display_name}\")\n",
        "    print(f\"📂 Training Data:   {train_uri}\")\n",
        "    print(f\"📂 Validation Data: {val_uri}\")\n",
        "\n",
        "    tuning_config_args = {\n",
        "        \"tuned_model_display_name\": display_name,\n",
        "        \"validation_dataset\": {\"gcs_uri\": val_uri}\n",
        "    }\n",
        "\n",
        "    if epochs:\n",
        "        tuning_config_args[\"epoch_count\"] = epochs\n",
        "    if learning_rate:\n",
        "        tuning_config_args[\"learning_rate_multiplier\"] = learning_rate\n",
        "\n",
        "    job = client.tunings.tune(\n",
        "        base_model=base_model,\n",
        "        training_dataset={\"gcs_uri\": train_uri},\n",
        "        config=types.CreateTuningJobConfig(**tuning_config_args),\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Job started! Name: {job.name}\")\n",
        "    return job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLlAgVjCNqXg"
      },
      "outputs": [],
      "source": [
        "tuning_job = client.tunings.get(name=sft_tuning_job.name)\n",
        "tuning_job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22QZ035C8GJ3"
      },
      "source": [
        "**Note: Tuning time depends on several factors, such as training data size, number of epochs, learning rate multiplier, etc.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1KX-_WyKeu"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ It will take ~15 mins for the model tuning job to complete on the provided dataset and set configurations/hyperparameters. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f3380996dc4"
      },
      "source": [
        "### Status Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ma3P6tZ6suI"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Wait for job completion\n",
        "\n",
        "running_states = [\n",
        "    \"JOB_STATE_PENDING\",\n",
        "    \"JOB_STATE_RUNNING\",\n",
        "]\n",
        "\n",
        "while tuning_job.state.name in running_states:\n",
        "    print(\".\", end=\"\")\n",
        "    tuning_job = client.tunings.get(name=tuning_job.name)\n",
        "    time.sleep(10)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1O1xCBS6spi"
      },
      "outputs": [],
      "source": [
        "tuned_model = tuning_job.tuned_model.endpoint\n",
        "experiment_name = tuning_job.experiment\n",
        "\n",
        "print(\"Tuned model experiment\", experiment_name)\n",
        "print(\"Tuned model endpoint resource name:\", tuned_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DzlWWKpbGcu"
      },
      "source": [
        "### Step7 [a]: Tuning and evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psRbCfzwWz_g"
      },
      "source": [
        "#### Model tuning metrics\n",
        "\n",
        "- `/train_total_loss`: Loss for the tuning dataset at a training step.\n",
        "- `/train_fraction_of_correct_next_step_preds`: The token accuracy at a training step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the tuning dataset.\n",
        "- `/train_num_predictions`: Number of predicted tokens at a training step\n",
        "\n",
        "#### Model evaluation metrics:\n",
        "\n",
        "- `/eval_total_loss`: Loss for the evaluation dataset at an evaluation step.\n",
        "- `/eval_fraction_of_correct_next_step_preds`: The token accuracy at an evaluation step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the evaluation dataset.\n",
        "- `/eval_num_predictions`: Number of predicted tokens at an evaluation step.\n",
        "\n",
        "The metrics visualizations are available after the model tuning job completes. If you don't specify a validation dataset when you create the tuning job, only the visualizations for the tuning metrics are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J1LP3nCbNlg"
      },
      "outputs": [],
      "source": [
        "# Locate Vertex AI Experiment and Vertex AI Experiment Run\n",
        "experiment = aiplatform.Experiment(experiment_name=experiment_name)\n",
        "filter_str = metadata_utils._make_filter_string(\n",
        "    schema_title=\"system.ExperimentRun\",\n",
        "    parent_contexts=[experiment.resource_name],\n",
        ")\n",
        "experiment_run = context.Context.list(filter_str)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htBrcQY1bPyh"
      },
      "outputs": [],
      "source": [
        "# Read data from Tensorboard\n",
        "tensorboard_run_name = f\"{experiment.get_backing_tensorboard_resource().resource_name}/experiments/{experiment.name}/runs/{experiment_run.name.replace(experiment.name, '')[1:]}\"\n",
        "tensorboard_run = aiplatform.TensorboardRun(tensorboard_run_name)\n",
        "metrics = tensorboard_run.read_time_series_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRZ-UZXcbYj5"
      },
      "outputs": [],
      "source": [
        "def get_metrics(metric: str = \"/train_total_loss\"):\n",
        "    \"\"\"Get metrics from Tensorboard.\n",
        "\n",
        "    Args:\n",
        "      metric: metric name, eg. /train_total_loss or /eval_total_loss.\n",
        "\n",
        "    Returns:\n",
        "      steps: list of steps.\n",
        "      steps_loss: list of loss values.\n",
        "    \"\"\"\n",
        "    loss_values = metrics[metric].values\n",
        "    steps_loss = []\n",
        "    steps = []\n",
        "    for loss in loss_values:\n",
        "        steps_loss.append(loss.scalar.value)\n",
        "        steps.append(loss.step)\n",
        "    return steps, steps_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImR4doLZblaH"
      },
      "outputs": [],
      "source": [
        "# Get Train and Eval Loss\n",
        "train_loss = get_metrics(metric=\"/train_total_loss\")\n",
        "eval_loss = get_metrics(metric=\"/eval_total_loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuN-m1Ikbn15"
      },
      "source": [
        "### Step7 [b]: Plot the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KWWkVR5jQkA"
      },
      "outputs": [],
      "source": [
        "# Plot the train and eval loss metrics using Plotly python library\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2, shared_xaxes=True, subplot_titles=(\"Train Loss\", \"Eval Loss\")\n",
        ")\n",
        "\n",
        "# Add traces\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=train_loss[0], y=train_loss[1], name=\"Train Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=1,\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=eval_loss[0], y=eval_loss[1], name=\"Eval Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=2,\n",
        ")\n",
        "\n",
        "# Add figure title\n",
        "fig.update_layout(title=\"Train and Eval Loss\", xaxis_title=\"Steps\", yaxis_title=\"Loss\")\n",
        "\n",
        "# Set x-axis title\n",
        "fig.update_xaxes(title_text=\"Steps\")\n",
        "\n",
        "# Set y-axes titles\n",
        "fig.update_yaxes(title_text=\"Loss\")\n",
        "\n",
        "# Show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY-eiVk0FI-M"
      },
      "source": [
        "## Step8: Load the Tuned Model\n",
        "\n",
        " - Load the fine-tuned model using `GenerativeModel` class with the tuning job model endpoint name.\n",
        "\n",
        " - Test the tuned model with the following prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av7MFIfltCMe"
      },
      "outputs": [],
      "source": [
        "sft_endpoint_name = \"projects/462579788136/locations/us-central1/endpoints/8345274563122167808\"\n",
        "\n",
        "sft_results = run_evaluation(\n",
        "    test_df=test_df,\n",
        "    model_name=sft_endpoint_name,\n",
        "    prompt_template=PROMPT_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U5K2qL2y3vl"
      },
      "outputs": [],
      "source": [
        "print(\"Plotting SFT Model Results...\")\n",
        "plot_confusion_matrix(sft_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdJbDgdJzbgd"
      },
      "outputs": [],
      "source": [
        "sft_errors = analyze_model_errors(\n",
        "    sft_results,\n",
        "    output_csv_path=\"sft_errors_test data.csv\",\n",
        "    top_n=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_by_query_segment(sft_results)"
      ],
      "metadata": {
        "id": "uj7qHIxrkl39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "katYxTQQegW1"
      },
      "source": [
        "##Step9:DPO preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqQJDlCQP5X7"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. Prepare Data and Model\n",
        "# ==========================================\n",
        "\n",
        "# Attempt to read the training dataset CSV generated in Step 3\n",
        "try:\n",
        "    # Ensure we are reading the training set (as we want to find Hard Cases the model couldn't learn)\n",
        "    train_data_df = pd.read_csv(\"experiment_en_train_samples\")\n",
        "    print(f\"✅ Successfully loaded training dataset: {len(train_data_df)} records\")\n",
        "except FileNotFoundError:\n",
        "    print(\"⚠️ Could not find 'experiment_en_train_samples'. Please confirm if Step 3 has been executed.\")\n",
        "    # If the variable is still in memory, we can use train_df directly\n",
        "    if 'train_df' in locals():\n",
        "        train_data_df = train_df\n",
        "        print(f\"✅ Directly using train_df from memory: {len(train_data_df)} records\")\n",
        "\n",
        "# Set your SFT Model Endpoint (Please confirm this is the Endpoint trained in Step 8)\n",
        "# This is the Endpoint found from your provided Log; please verify based on your actual situation\n",
        "sft_endpoint_name = \"projects/462579788136/locations/us-central1/endpoints/8345274563122167808\"\n",
        "\n",
        "# ==========================================\n",
        "# 2. Run Evaluation (Run Evaluation on Train Data)\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🚀 Starting prediction on Training Data (Searching for DPO candidate samples)...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Use your defined function to run prediction\n",
        "train_results = run_evaluation(\n",
        "    test_df=train_data_df,       # Pass in training data here\n",
        "    model_name=sft_endpoint_name, # Use the model after SFT\n",
        "    prompt_template=PROMPT_TEMPLATE\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# 3. Visualize Results (Confusion Matrix)\n",
        "# ==========================================\n",
        "print(\"\\n📊 Plotting Confusion Matrix...\")\n",
        "plot_confusion_matrix(train_results)\n",
        "\n",
        "# ==========================================\n",
        "# 4. Error Analysis and DPO Data Export (Analyze Errors)\n",
        "# ==========================================\n",
        "print(\"\\n🔍 Analyzing errors and exporting DPO candidate data...\")\n",
        "\n",
        "# This will find all cases where model_prediction != ground_truth\n",
        "# These are the best materials for DPO:\n",
        "#   - Prompt: search_query + product_name\n",
        "#   - Chosen (Win): ground_truth\n",
        "#   - Rejected (Loss): model_prediction (because this is what the model got wrong)\n",
        "\n",
        "dpo_candidates_df = analyze_model_errors(\n",
        "    results_dict=train_results,\n",
        "    output_csv_path=\"dpo_candidates_from_train.csv\", # Filename for saving\n",
        "    top_n=10 # Show top 10 severe errors\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ DPO preparation complete! Found {len(dpo_candidates_df)} error samples in total.\")\n",
        "print(\"You can use 'dpo_candidates_from_train.csv' to create the JSONL file for Preference Tuning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJVHTM5gDRIP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the filename\n",
        "csv_filename = \"sft_error_before dpo_train data.csv\"\n",
        "\n",
        "# Check if file exists before loading\n",
        "if os.path.exists(csv_filename):\n",
        "    # 1. Load the CSV\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    # 2. Filter for rows where 'T/F' is 'T'\n",
        "    # We strip whitespace just in case there are hidden spaces\n",
        "    df_filtered = df[df['T/F'].astype(str).str.strip() == 'T'].copy()\n",
        "\n",
        "    print(f\"✅ CSV loaded successfully.\")\n",
        "    print(f\"Total rows in original file: {len(df)}\")\n",
        "    print(f\"Rows selected for training (T/F = T): {len(df_filtered)}\")\n",
        "\n",
        "    # Display the first few rows to verify columns\n",
        "    print(\"\\nSample of filtered data:\")\n",
        "    display(df_filtered.head(3))\n",
        "\n",
        "else:\n",
        "    print(f\"❌ Error: Could not find file '{csv_filename}'. Please ensure it is uploaded to the Colab runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVQU8eFwkID7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def generate_gemini_dpo_dataset(input_df, prompt_template, output_filename):\n",
        "    \"\"\"\n",
        "    Transforms a DataFrame into Gemini Preference Tuning (DPO) JSONL format.\n",
        "\n",
        "    Parameters:\n",
        "    1. input_df (pd.DataFrame): Data containing 'search_query', 'product_name_local', 'result', 'model_prediction'.\n",
        "    2. prompt_template (str): A string with '{query}' and '{product}' placeholders.\n",
        "    3. output_filename (str): The path to save the JSONL file.\n",
        "\n",
        "    Returns:\n",
        "    list: The list of dictionary objects generated.\n",
        "    \"\"\"\n",
        "\n",
        "    dpo_dataset = []\n",
        "    print(\"🔄 Transforming data...\")\n",
        "\n",
        "    for index, row in input_df.iterrows():\n",
        "        try:\n",
        "            # 1. Prepare dynamic variables\n",
        "            query_val = str(row['search_query'])\n",
        "            product_val = str(row['product_name_local'])\n",
        "\n",
        "            # 2. Fill the prompt template\n",
        "            user_prompt = prompt_template.format(query=query_val, product=product_val)\n",
        "\n",
        "            # 3. Get the Chosen (Ground Truth) and Rejected (Model Error)\n",
        "            chosen_text = str(row['result'])\n",
        "            rejected_text = str(row['model_prediction'])\n",
        "\n",
        "            # 4. Build the Gemini DPO dictionary object\n",
        "            entry = {\n",
        "                \"contents\": [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"parts\": [{\"text\": user_prompt}]\n",
        "                    }\n",
        "                ],\n",
        "                \"completions\": [\n",
        "                    # Chosen response (Score 1.0)\n",
        "                    {\n",
        "                        \"score\": 1.0,\n",
        "                        \"completion\": {\n",
        "                            \"role\": \"model\",\n",
        "                            \"parts\": [{\"text\": chosen_text}]\n",
        "                        }\n",
        "                    },\n",
        "                    # Rejected response (Score 0.0)\n",
        "                    {\n",
        "                        \"score\": 0.0,\n",
        "                        \"completion\": {\n",
        "                            \"role\": \"model\",\n",
        "                            \"parts\": [{\"text\": rejected_text}]\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "            dpo_dataset.append(entry)\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"Skipping row {index}: Missing column {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error at index {index}: {e}\")\n",
        "\n",
        "    # 5. Save to JSONL file\n",
        "    if dpo_dataset:\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            for entry in dpo_dataset:\n",
        "                f.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "        print(f\"✅ Transformation complete!\")\n",
        "        print(f\"Saved {len(dpo_dataset)} examples to '{output_filename}'\")\n",
        "\n",
        "        # 6. Output the first line for verification (Requirement #4)\n",
        "        print(\"\\n🔍 Preview of the first JSONL entry:\")\n",
        "        print(json.dumps(dpo_dataset[0], indent=2))\n",
        "    else:\n",
        "        print(\"⚠️ No data was transformed. Please check your input DataFrame.\")\n",
        "\n",
        "    return dpo_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXKB9-7Q0N3_"
      },
      "outputs": [],
      "source": [
        "DPO_TEMPLATE = \"\"\"You are a search relevance expert. Your task is to label ecommerce products relevance for a given query.\n",
        "Output ONLY the single word label.\n",
        "\n",
        "Query: {query}\n",
        "Product: {product}\n",
        "Result:\"\"\"\n",
        "\n",
        "dpo_data = generate_gemini_dpo_dataset(\n",
        "    input_df=df_filtered,\n",
        "    prompt_template=DPO_TEMPLATE,\n",
        "    output_filename=\"gemini_dpo_training_data.jsonl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oYJGOdWnB2V"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "def prepare_and_upload_to_gcs(data_list, gcs_bucket_path, val_ratio=0.15, random_seed=42):\n",
        "    \"\"\"\n",
        "    Splits the data into training and validation sets, saves them locally,\n",
        "    and uploads them to Google Cloud Storage.\n",
        "\n",
        "    Parameters:\n",
        "    1. data_list (list): The list of DPO data dictionaries (from the previous step).\n",
        "    2. gcs_bucket_path (str): The GCS folder path (e.g., \"gs://my-bucket/folder\").\n",
        "    3. val_ratio (float): The proportion of data to use for validation (default 0.15).\n",
        "    4. random_seed (int): Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    tuple: (train_gcs_uri, val_gcs_uri) if successful, else (None, None).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Validation Check\n",
        "    if not data_list:\n",
        "        print(\"❌ Error: Input data_list is empty.\")\n",
        "        return None, None\n",
        "\n",
        "    total_count = len(data_list)\n",
        "    print(f\"Total data points: {total_count}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. Perform Split (Train / Val)\n",
        "    # ==========================================\n",
        "    # Shuffle data order\n",
        "    random.seed(random_seed)\n",
        "    # Create a copy to avoid modifying the original list outside the function\n",
        "    shuffled_data = data_list.copy()\n",
        "    random.shuffle(shuffled_data)\n",
        "\n",
        "    # Calculate validation set size\n",
        "    val_count = int(total_count * val_ratio)\n",
        "\n",
        "    # Safety check: Ensure at least 1 validation item, and training data is not empty\n",
        "    if val_count < 1 and total_count > 1:\n",
        "        val_count = 1\n",
        "    elif val_count >= total_count:\n",
        "        val_count = total_count - 1 # Keep at least one item for training\n",
        "\n",
        "    # Split the lists\n",
        "    val_data = shuffled_data[:val_count]\n",
        "    train_data = shuffled_data[val_count:]\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Split Strategy: {int(val_ratio*100)}% Validation\")\n",
        "    print(f\"Training set size:   {len(train_data)}\")\n",
        "    print(f\"Validation set size: {len(val_data)}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. Save as temporary local JSONL files\n",
        "    # ==========================================\n",
        "    # We still need to save files locally because `gsutil` uploads from file system\n",
        "    train_filename = \"train_data.jsonl\"\n",
        "    val_filename = \"val_data.jsonl\"\n",
        "\n",
        "    def save_jsonl(filename, content):\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            for entry in content:\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "        print(f\"Saved local file: {filename}\")\n",
        "\n",
        "    save_jsonl(train_filename, train_data)\n",
        "    save_jsonl(val_filename, val_data)\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. Upload to Google Cloud Storage\n",
        "    # ==========================================\n",
        "    print(f\"\\n📤 Uploading files to: {gcs_bucket_path} ...\")\n",
        "\n",
        "    # Remove trailing slash from path if present to avoid double slashes\n",
        "    gcs_bucket_path = gcs_bucket_path.rstrip('/')\n",
        "\n",
        "    # Use os.system to execute gsutil commands\n",
        "    ret_train = os.system(f\"gsutil cp {train_filename} {gcs_bucket_path}/{train_filename}\")\n",
        "    ret_val = os.system(f\"gsutil cp {val_filename} {gcs_bucket_path}/{val_filename}\")\n",
        "\n",
        "    if ret_train == 0 and ret_val == 0:\n",
        "        train_uri = f\"{gcs_bucket_path}/{train_filename}\"\n",
        "        val_uri = f\"{gcs_bucket_path}/{val_filename}\"\n",
        "\n",
        "        print(\"\\n✅ Upload successful! Use these URIs for training:\")\n",
        "        print(f\"Training data:   {train_uri}\")\n",
        "        print(f\"Validation data: {val_uri}\")\n",
        "\n",
        "        return train_uri, val_uri\n",
        "    else:\n",
        "        print(\"\\n❌ Upload failed. Please check your GCS path and permissions.\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DXorLkt2Ax-"
      },
      "outputs": [],
      "source": [
        "train_uri, val_uri = prepare_and_upload_to_gcs(\n",
        "    data_list=dpo_data,\n",
        "    gcs_bucket_path=bucket_path\n",
        ")\n",
        "\n",
        "if train_uri:\n",
        "    print(f\"Ready to train with: {train_uri}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg1HPI3pvcJO"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "SFT_MODEL_URI = \"projects/462579788136/locations/us-central1/models/1047921442689122304\"\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1.0\n",
        "ADAPTER_SIZE = \"ADAPTER_SIZE_FOUR\"\n",
        "BETA = 0.1\n",
        "\n",
        "print(f\"Base Model: {SFT_MODEL_URI}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Adapter Size: {ADAPTER_SIZE}\")\n",
        "print(f\"Beta: {BETA}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sDMhqwfvnQq"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "\n",
        "def submit_dpo_tuning_job(client, base_model_uri, train_uri, val_uri,\n",
        "                          display_name=\"gemini-dpo-tuning-v1\",\n",
        "                          epochs=4, adapter_size=4, learning_rate=1.0, beta=0.1,\n",
        "                          project_id=\"462579788136\", location=\"us-central1\"):\n",
        "    \"\"\"\n",
        "    Submits a Gemini Preference Tuning (DPO) job to Vertex AI (Fixed for SDK v1.0+).\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"🚀 Submitting tuning job: {display_name} ...\\n\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. Fix Adapter Size (Int -> Enum String)\n",
        "    # ==========================================\n",
        "    # Map integer input to the required API Enum string\n",
        "    adapter_size_map = {\n",
        "        1: \"ADAPTER_SIZE_ONE\",\n",
        "        4: \"ADAPTER_SIZE_FOUR\",\n",
        "        8: \"ADAPTER_SIZE_EIGHT\",\n",
        "        16: \"ADAPTER_SIZE_SIXTEEN\"\n",
        "    }\n",
        "\n",
        "    # Get the string value, default to 4 if not found\n",
        "    final_adapter_size = adapter_size_map.get(adapter_size, \"ADAPTER_SIZE_FOUR\")\n",
        "    print(f\"   Using Adapter Size: {final_adapter_size} (mapped from {adapter_size})\")\n",
        "\n",
        "    try:\n",
        "        # ==========================================\n",
        "        # 2. Fix Dataset Types\n",
        "        # ==========================================\n",
        "        # Training uses TuningDataset\n",
        "        training_dataset = types.TuningDataset(\n",
        "            gcs_uri=train_uri\n",
        "        )\n",
        "\n",
        "        # Validation MUST use TuningValidationDataset (Fixing the Type Mismatch warning)\n",
        "        validation_dataset = types.TuningValidationDataset(\n",
        "            gcs_uri=val_uri\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # 3. Submit the tuning job\n",
        "        # ==========================================\n",
        "        tuning_job = client.tunings.tune(\n",
        "            base_model=base_model_uri,\n",
        "            training_dataset=training_dataset,\n",
        "            config=types.CreateTuningJobConfig(\n",
        "                tuned_model_display_name=display_name,\n",
        "                method=\"PREFERENCE_TUNING\",\n",
        "                epoch_count=epochs,\n",
        "                adapter_size=final_adapter_size,  # ✅ Passing string now\n",
        "                learning_rate_multiplier=learning_rate,\n",
        "                beta=beta,\n",
        "                validation_dataset=validation_dataset, # ✅ Passing correct type\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # 4. Get job information\n",
        "        job_name = tuning_job.name\n",
        "        job_id = job_name.split(\"/\")[-1] if \"/\" in job_name else job_name\n",
        "\n",
        "        print(\"✅ Tuning job submitted successfully!\\n\")\n",
        "        print(f\"Job Name: {job_name}\")\n",
        "        print(f\"Job ID:   {job_id}\\n\")\n",
        "\n",
        "        # 5. Generate Monitoring Link\n",
        "        if project_id:\n",
        "            monitor_url = f\"https://console.cloud.google.com/vertex-ai/tuning/locations/{location}/tuningJob/{job_id}/monitor?project={project_id}\"\n",
        "            print(\"Monitor progress in the console:\")\n",
        "            print(monitor_url)\n",
        "\n",
        "        print(\"\\n⏱️ Training will take approximately 30-60 minutes.\")\n",
        "\n",
        "        return tuning_job\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error submitting job: {e}\")\n",
        "        # Print detailed error if available\n",
        "        if hasattr(e, 'details'):\n",
        "            print(f\"Details: {e.details()}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfCdCEfe4JoS"
      },
      "outputs": [],
      "source": [
        "dpo_job = submit_dpo_tuning_job(\n",
        "    client=client,\n",
        "    base_model_uri=SFT_MODEL_URI,\n",
        "    train_uri=train_uri,\n",
        "    val_uri=val_uri,\n",
        "    display_name=\"en-gemini-dpo-experiment-01\",\n",
        "    project_id=\"462579788136\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJRtc9wwyxU7"
      },
      "outputs": [],
      "source": [
        "# Step 6: Check tuning job status\n",
        "import time\n",
        "\n",
        "# ✅ I have automatically filled in the job name generated from the previous step\n",
        "JOB_NAME = \"projects/462579788136/locations/us-central1/tuningJobs/2401672508500606976\"\n",
        "\n",
        "if not JOB_NAME or JOB_NAME == \"[your_job_name]\":\n",
        "    print(\"⚠️ Please set JOB_NAME above with the value from Step 5\")\n",
        "else:\n",
        "    try:\n",
        "        # Query the current job status from Vertex AI\n",
        "        tuning_job = client.tunings.get(name=JOB_NAME)\n",
        "        state = tuning_job.state\n",
        "\n",
        "        print(f\"📊 Current Status: {state}\\n\")\n",
        "\n",
        "        # Provide feedback based on the status\n",
        "        if state == \"JOB_STATE_PENDING\":\n",
        "            print(\"⏳ Job is queued, waiting for compute resources...\")\n",
        "            print(\"   (Queued: Waiting for Google to allocate resources. This usually takes 2-5 minutes.)\")\n",
        "            print(\"   Please re-run this cell in a few minutes to check again.\")\n",
        "\n",
        "        elif state == \"JOB_STATE_RUNNING\":\n",
        "            print(\"🔄 Training is in progress!\")\n",
        "            print(\"   (Training: The model is learning from your data.)\")\n",
        "            print(\"   This typically takes 30-60 minutes. Feel free to take a break and check back later.\")\n",
        "\n",
        "        elif state == \"JOB_STATE_SUCCEEDED\":\n",
        "            print(\"✅ Training completed successfully!\\n\")\n",
        "            print(\"🎯 Your tuned model is ready to use!\")\n",
        "            print(\"   (Training complete! You can proceed to Step 7 to test the model.)\")\n",
        "\n",
        "            # Display model information\n",
        "            if hasattr(tuning_job, \"tuned_model\") and tuning_job.tuned_model:\n",
        "                if hasattr(tuning_job.tuned_model, \"endpoint\"):\n",
        "                    print(f\"📝 Endpoint: {tuning_job.tuned_model.endpoint}\")\n",
        "\n",
        "        elif state == \"JOB_STATE_FAILED\":\n",
        "            print(\"❌ Training failed.\\n\")\n",
        "            if hasattr(tuning_job, \"error\"):\n",
        "                print(\"Error details:\")\n",
        "                print(tuning_job.error)\n",
        "\n",
        "        else:\n",
        "            print(f\"❓ Unknown state: {state}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking job status: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KG_8lm_4y-j"
      },
      "source": [
        "Test again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "639ygJwX5Gwn"
      },
      "outputs": [],
      "source": [
        "DPO_tuned_model = \"projects/462579788136/locations/us-central1/endpoints/6013958068516159488\"\n",
        "\n",
        "en_DPO_results = run_evaluation(\n",
        "    test_df=test_df,\n",
        "    model_name=DPO_tuned_model,\n",
        "    prompt_template=PROMPT_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zibiU2hl589F"
      },
      "outputs": [],
      "source": [
        "print(\"Plotting DPO+SFT Model Results...\")\n",
        "plot_confusion_matrix(en_DPO_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bN8QQMh6Jyd"
      },
      "outputs": [],
      "source": [
        "sft_errors = analyze_model_errors(\n",
        "    en_DPO_results,\n",
        "    output_csv_path=\"sft_error_after dpo_test data.csv\",\n",
        "    top_n=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iET5YlFd9q-y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# 1. Define Data\n",
        "n1 = 1192\n",
        "p1 = 0.9237\n",
        "n2 = 1192\n",
        "p2 = 0.9169\n",
        "\n",
        "# Calculate number of successes (Required as integers for the test)\n",
        "count1 = round(p1 * n1)\n",
        "count2 = round(p2 * n2)\n",
        "\n",
        "print(f\"Pre-tuning successes: {count1}, Sample size: {n1}\")\n",
        "print(f\"Post-tuning successes: {count2}, Sample size: {n2}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# --- Method 1: Using statsmodels (Recommended) ---\n",
        "# 'larger' means we are testing if the proportion in the first array (Post)\n",
        "# is greater than the second array (Pre).\n",
        "counts = np.array([count2, count1])  # Order: [Post-tuning, Pre-tuning]\n",
        "nobs = np.array([n2, n1])\n",
        "z_stat, p_val = proportions_ztest(counts, nobs, alternative='larger')\n",
        "\n",
        "print(\"[Method 1: statsmodels Results]\")\n",
        "print(f\"Z-score: {z_stat:.4f}\")\n",
        "print(f\"P-value: {p_val:.4f}\")\n",
        "\n",
        "# --- Conclusion ---\n",
        "alpha = 0.05\n",
        "print(\"-\" * 35)\n",
        "if p_val < alpha:\n",
        "    print(f\"Conclusion: P-value < {alpha}. Result is statistically significant!\")\n",
        "    print(\"The fine-tuned model is significantly better.\")\n",
        "else:\n",
        "    print(f\"Conclusion: P-value > {alpha}. Result is NOT statistically significant.\")\n",
        "    print(\"The improvement may be due to random chance.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}